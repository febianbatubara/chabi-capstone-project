{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yU2pQ_iEgLDl"
   },
   "source": [
    "## **Import Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2975,
     "status": "ok",
     "timestamp": 1623139689837,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "kxO3Nvw2usfN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Febian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Febian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Febian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Febian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Febian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Febian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Febian\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Febian\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Febian\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Febian\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Febian\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Febian\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\n",
    "# Ignore noise warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9RFURUXgEbc"
   },
   "source": [
    "## **Explore The Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "executionInfo": {
     "elapsed": 1729,
     "status": "ok",
     "timestamp": 1623139695518,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "j-vDwJf4t_fh",
    "outputId": "51b445d9-c6f3-4eb5-c01c-e70c4ece0468"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>ISFP</td>\n",
       "      <td>'https://www.youtube.com/watch?v=t8edHB_h908||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>'So...if this thread already exists someplace ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'So many questions when i do these things.  I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'I am very conflicted right now when it comes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'It has been too long since I have been on per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts\n",
       "8670  ISFP  'https://www.youtube.com/watch?v=t8edHB_h908||...\n",
       "8671  ENFP  'So...if this thread already exists someplace ...\n",
       "8672  INTP  'So many questions when i do these things.  I ...\n",
       "8673  INFP  'I am very conflicted right now when it comes ...\n",
       "8674  INFP  'It has been too long since I have been on per..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading dataset\n",
    "data_set = pd.read_csv(\"mbti_1.csv\")\n",
    "data_set.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1623139698895,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "bYZn-L45xWTj",
    "outputId": "d6191828-3154-463f-aed4-cdfc2dc31df2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP', 'ESTJ', 'ESTP',\n",
       "       'INFJ', 'INFP', 'INTJ', 'INTP', 'ISFJ', 'ISFP', 'ISTJ', 'ISTP'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types = np.unique(np.array(data_set['type']))\n",
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1623139701878,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "rMoNqTBfxeqh",
    "outputId": "ece67ef0-c5c4-43ef-d8cf-f7a29baea35e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENFJ</th>\n",
       "      <td>9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENFP</th>\n",
       "      <td>33750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTJ</th>\n",
       "      <td>11550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTP</th>\n",
       "      <td>34250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFJ</th>\n",
       "      <td>2100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFP</th>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTJ</th>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTP</th>\n",
       "      <td>4450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFJ</th>\n",
       "      <td>73500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFP</th>\n",
       "      <td>91600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>54550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTP</th>\n",
       "      <td>65200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFJ</th>\n",
       "      <td>8300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFP</th>\n",
       "      <td>13550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTJ</th>\n",
       "      <td>10250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTP</th>\n",
       "      <td>16850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      posts\n",
       "type       \n",
       "ENFJ   9500\n",
       "ENFP  33750\n",
       "ENTJ  11550\n",
       "ENTP  34250\n",
       "ESFJ   2100\n",
       "ESFP   2400\n",
       "ESTJ   1950\n",
       "ESTP   4450\n",
       "INFJ  73500\n",
       "INFP  91600\n",
       "INTJ  54550\n",
       "INTP  65200\n",
       "ISFJ   8300\n",
       "ISFP  13550\n",
       "ISTJ  10250\n",
       "ISTP  16850"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = data_set.groupby(['type']).count()*50\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4841,
     "status": "ok",
     "timestamp": 1623139737867,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "N-08lBTmxmeZ",
    "outputId": "653e242d-eda6-4247-c6a5-4388a7081c43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 387957),\n",
       " ('to', 290168),\n",
       " ('the', 270699),\n",
       " ('a', 230918),\n",
       " ('and', 219498),\n",
       " ('of', 177853),\n",
       " ('is', 128804),\n",
       " ('you', 128750),\n",
       " ('that', 127221),\n",
       " ('in', 117263),\n",
       " ('my', 104561),\n",
       " ('it', 93101),\n",
       " ('for', 83057),\n",
       " ('have', 79784),\n",
       " ('with', 77131),\n",
       " ('but', 74729),\n",
       " ('be', 69317),\n",
       " ('are', 65034),\n",
       " ('like', 61390),\n",
       " ('not', 59496),\n",
       " ('an', 59020),\n",
       " (\"I'm\", 57339),\n",
       " ('on', 57062),\n",
       " ('was', 56146),\n",
       " ('me', 55488),\n",
       " ('as', 53310),\n",
       " ('this', 52601),\n",
       " ('just', 48292),\n",
       " ('about', 46305),\n",
       " ('think', 46229),\n",
       " ('or', 45724),\n",
       " (\"don't\", 44821),\n",
       " ('so', 42935),\n",
       " ('your', 40918),\n",
       " ('do', 40867),\n",
       " ('what', 37746),\n",
       " ('at', 37566),\n",
       " ('can', 37535),\n",
       " ('if', 37042),\n",
       " ('people', 35546)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_set.copy()\n",
    "#Finding the most common words in all posts.\n",
    "words = list(df[\"posts\"].apply(lambda x: x.split()))\n",
    "words = [x for y in words for x in y]\n",
    "Counter(words).most_common(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg0oD9jzgdCG"
   },
   "source": [
    "## **Preparing the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2214,
     "status": "ok",
     "timestamp": 1623139742904,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "22Lj1s3Dx3d-",
    "outputId": "2da9230c-34fd-4ceb-bcda-87fef51f846b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users 8675\n",
      "Number of posts 422845\n",
      "5 posts from start are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('INFJ', \"'http://www.youtube.com/watch?v=qsXHcwe3krw\"),\n",
       " ('INFJ', 'http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg'),\n",
       " ('INFJ',\n",
       "  'enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not top ten plays  https://www.youtube.com/watch?v=uCdfze1etec  pranks'),\n",
       " ('INFJ', 'What has been the most life-changing experience in your life?'),\n",
       " ('INFJ',\n",
       "  'http://www.youtube.com/watch?v=vXZeYwwRDw8   http://www.youtube.com/watch?v=u8ejam5DP3E  On repeat for most of today.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract(posts, new_posts):\n",
    "    for post in posts[1].split(\"|||\"):\n",
    "        new_posts.append((posts[0], post))\n",
    "\n",
    "posts = []\n",
    "df.apply(lambda x: extract(x, posts), axis=1)\n",
    "print(\"Number of users\", len(df))\n",
    "print(\"Number of posts\", len(posts))\n",
    "print(\"5 posts from start are:\")\n",
    "posts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 25972,
     "status": "ok",
     "timestamp": 1623139770940,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "8B2BWvQ802YV"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(df, remove_special=True):\n",
    "    texts = df['posts'].copy()\n",
    "    labels = df['type'].copy()\n",
    "\n",
    "    #Remove links \n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'https?:\\/\\/.*?[\\s+]', '', x.replace(\"|\",\" \") + \" \"))\n",
    "    \n",
    "    #Keep the End Of Sentence characters\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', x + \" \"))\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', x + \" \"))\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', x + \" \"))\n",
    "    \n",
    "    #Strip Punctation\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[\\.+]', \".\",x))\n",
    "\n",
    "    #Remove multiple fullstops\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
    "\n",
    "    #Remove Non-words\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','',x))\n",
    "\n",
    "    #Convert posts to lowercase\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: x.lower())\n",
    "\n",
    "    #Remove multiple letter repeating words\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','',x)) \n",
    "\n",
    "    #Remove very short or long words\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{0,3})?\\b','',x)) \n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{30,1000})?\\b','',x))\n",
    "\n",
    "    #Remove MBTI Personality Words - crutial in order to get valid model accuracy estimation for unseen data. \n",
    "    if remove_special:\n",
    "        pers_types = ['INFP' ,'INFJ', 'INTP', 'INTJ', 'ENTP', 'ENFP', 'ISTP' ,'ISFP' ,'ENTJ', 'ISTJ','ENFJ', 'ISFJ' ,'ESTP', 'ESFP' ,'ESFJ' ,'ESTJ']\n",
    "        pers_types = [p.lower() for p in pers_types]\n",
    "        p = re.compile(\"(\" + \"|\".join(pers_types) + \")\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Preprocessing of entered Text\n",
    "new_df = preprocess_text(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1623139770951,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "oHbPDEvl1UzP",
    "outputId": "87fe1d4f-3b22-4d2c-ddf6-c3ca6a61b79a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp  intj moments   sportscenter    plays...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>finding  lack    these posts very alarming eo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>good         course  which    know thats  bles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>dear intp    enjoyed  conversation  other  eos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>youre fired eostokendot    thats another silly...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ      enfp  intj moments   sportscenter    plays...\n",
       "1  ENTP   finding  lack    these posts very alarming eo...\n",
       "2  INTP  good         course  which    know thats  bles...\n",
       "3  INTJ  dear intp    enjoyed  conversation  other  eos...\n",
       "4  ENTJ  youre fired eostokendot    thats another silly..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sax51gMugmUB"
   },
   "source": [
    "### Encoding the MBTI Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1623139780118,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "sDoWTfKm1sHr"
   },
   "outputs": [],
   "source": [
    "# Converting MBTI personality (or target or Y feature) into numerical form using Label Encoding\n",
    "# encoding personality type\n",
    "enc = LabelEncoder()\n",
    "new_df['type of encoding'] = enc.fit_transform(new_df['type'])\n",
    "\n",
    "target = new_df['type of encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1623139781270,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "ezU9x-5B2HWt",
    "outputId": "d384f572-ee6a-4827-eaa5-2c5555ae9cdf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>type of encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp  intj moments   sportscenter    plays...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>finding  lack    these posts very alarming eo...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>good         course  which    know thats  bles...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>dear intp    enjoyed  conversation  other  eos...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>youre fired eostokendot    thats another silly...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>eostokendot    science    perfect eostokendo...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>cant draw    nails haha eostokendot  those w...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>tend  build   collection  things   desktop th...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>sure thats  good question eostokendot   dist...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>INTP</td>\n",
       "      <td>this position where  have  actually     pe...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>time  parents were fighting over  dads affair...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>went through  break  some months  eosto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>santagato  entp   enfj   entp eostokenquest  ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>fair enough  thats   want  look   eostokendot ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>INTP</td>\n",
       "      <td>basically this eostokendot  eostokendot  eosto...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type                                              posts  type of encoding\n",
       "0   INFJ      enfp  intj moments   sportscenter    plays...                 8\n",
       "1   ENTP   finding  lack    these posts very alarming eo...                 3\n",
       "2   INTP  good         course  which    know thats  bles...                11\n",
       "3   INTJ  dear intp    enjoyed  conversation  other  eos...                10\n",
       "4   ENTJ  youre fired eostokendot    thats another silly...                 2\n",
       "5   INTJ    eostokendot    science    perfect eostokendo...                10\n",
       "6   INFJ    cant draw    nails haha eostokendot  those w...                 8\n",
       "7   INTJ   tend  build   collection  things   desktop th...                10\n",
       "8   INFJ    sure thats  good question eostokendot   dist...                 8\n",
       "9   INTP      this position where  have  actually     pe...                11\n",
       "10  INFJ   time  parents were fighting over  dads affair...                 8\n",
       "11  ENFJ         went through  break  some months  eosto...                 0\n",
       "12  INFJ   santagato  entp   enfj   entp eostokenquest  ...                 8\n",
       "13  INTJ  fair enough  thats   want  look   eostokendot ...                10\n",
       "14  INTP  basically this eostokendot  eostokendot  eosto...                11"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1623139805015,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "2QECjEpE2Bh5",
    "outputId": "f319d69e-36d3-49d6-954b-75a69b753fd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Febian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "# The python natural language toolkit library provides a list of english stop words.\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erjBDXSuhJYY"
   },
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5195,
     "status": "ok",
     "timestamp": 1623139816123,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "ak3xCyo_2qly"
   },
   "outputs": [],
   "source": [
    "# Vectorizing the posts for the model and filtering Stop-words\n",
    "vect = CountVectorizer(stop_words='english') \n",
    "\n",
    "# Converting posts (or training or X feature) into numerical form by count vectorization\n",
    "train =  vect.fit_transform(new_df[\"posts\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pf7IwQg_hNzP"
   },
   "source": [
    "## **Model Training and Evaluation**\n",
    "\n",
    "Split Training - Test into 70 - 30\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 640,
     "status": "ok",
     "timestamp": 1623139900050,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "GJPsHJr72wrz",
    "outputId": "89780718-9d95-4995-eb1d-139a609da4e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5812, 98580) (5812,) (2863, 98580) (2863,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.33, stratify=target, random_state=42)\n",
    "print ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41319,
     "status": "ok",
     "timestamp": 1623139941355,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "ozMIG76-3g-i",
    "outputId": "f751a54f-a4a6-4efc-a6c7-3f97b4329c5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.11%\n"
     ]
    }
   ],
   "source": [
    "accuracies = {}\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "Y_pred = logreg.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracies['Logistic Regression'] = accuracy* 100.0\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lLsyu2ghlKs"
   },
   "source": [
    "#### **Pre-Processing**\n",
    "for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "executionInfo": {
     "elapsed": 4611,
     "status": "ok",
     "timestamp": 1623140012349,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "XvbkWoPw3weT",
    "outputId": "05a3201c-d1b0-4eff-c157-7b307c00eb94"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>type of encoding</th>\n",
       "      <th>IE</th>\n",
       "      <th>NS</th>\n",
       "      <th>TF</th>\n",
       "      <th>JP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp  intj moments   sportscenter    plays...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>finding  lack    these posts very alarming eo...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>good         course  which    know thats  bles...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>dear intp    enjoyed  conversation  other  eos...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>youre fired eostokendot    thats another silly...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  type of encoding  \\\n",
       "0  INFJ      enfp  intj moments   sportscenter    plays...                 8   \n",
       "1  ENTP   finding  lack    these posts very alarming eo...                 3   \n",
       "2  INTP  good         course  which    know thats  bles...                11   \n",
       "3  INTJ  dear intp    enjoyed  conversation  other  eos...                10   \n",
       "4  ENTJ  youre fired eostokendot    thats another silly...                 2   \n",
       "\n",
       "   IE  NS  TF  JP  \n",
       "0   1   1   0   1  \n",
       "1   0   1   1   0  \n",
       "2   1   1   1   0  \n",
       "3   1   1   1   1  \n",
       "4   0   1   1   1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_set\n",
    "def get_types(row):\n",
    "    t=row['type']\n",
    "\n",
    "    I = 0; N = 0\n",
    "    T = 0; J = 0\n",
    "    \n",
    "    if t[0] == 'I': I = 1\n",
    "    elif t[0] == 'E': I = 0\n",
    "    else: print('I-E not found') \n",
    "        \n",
    "    if t[1] == 'N': N = 1\n",
    "    elif t[1] == 'S': N = 0\n",
    "    else: print('N-S not found')\n",
    "        \n",
    "    if t[2] == 'T': T = 1\n",
    "    elif t[2] == 'F': T = 0\n",
    "    else: print('T-F not found')\n",
    "        \n",
    "    if t[3] == 'J': J = 1\n",
    "    elif t[3] == 'P': J = 0\n",
    "    else: print('J-P not found')\n",
    "    return pd.Series( {'IE':I, 'NS':N , 'TF': T, 'JP': J }) \n",
    "\n",
    "data = data.join(data.apply (lambda row: get_types (row),axis=1))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1008,
     "status": "ok",
     "timestamp": 1623140017340,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "zTBxCzXiKY2d",
    "outputId": "f3978889-9927-4dcc-aac9-b5dfac84e821"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introversion (I) /  Extroversion (E):\t 1999  /  6676\n",
      "Intuition (N) / Sensing (S):\t\t 1197  /  7478\n",
      "Thinking (T) / Feeling (F):\t\t 4694  /  3981\n",
      "Judging (J) / Perceiving (P):\t\t 5241  /  3434\n"
     ]
    }
   ],
   "source": [
    "print (\"Introversion (I) /  Extroversion (E):\\t\", data['IE'].value_counts()[0], \" / \", data['IE'].value_counts()[1])\n",
    "print (\"Intuition (N) / Sensing (S):\\t\\t\", data['NS'].value_counts()[0], \" / \", data['NS'].value_counts()[1])\n",
    "print (\"Thinking (T) / Feeling (F):\\t\\t\", data['TF'].value_counts()[0], \" / \", data['TF'].value_counts()[1])\n",
    "print (\"Judging (J) / Perceiving (P):\\t\\t\", data['JP'].value_counts()[0], \" / \", data['JP'].value_counts()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1623140025871,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "YGKRlfY_Laan"
   },
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "# Remove the stop words for speed \n",
    "useless_words = stopwords.words(\"english\")\n",
    "\n",
    "# Remove these from the posts\n",
    "unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "unique_type_list = [x.lower() for x in unique_type_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 757,
     "status": "ok",
     "timestamp": 1623140049990,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "NUADyOQ8Lc7V",
    "outputId": "43b83058-5d9d-488f-9b04-81d1b033edbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarize MBTI list: \n",
      "[[0 0 0 0]\n",
      " [1 0 1 1]\n",
      " [0 0 1 1]\n",
      " ...\n",
      " [0 0 1 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#Splitting the MBTI personality into 4 letters and binarizing it\n",
    "\n",
    "Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n",
    "\n",
    "def translate_personality(personality):\n",
    "    # transform mbti to binary vector\n",
    "    return [Pers[l] for l in personality]\n",
    "\n",
    "#To show result output for personality prediction\n",
    "def translate_back(personality):\n",
    "    # transform binary vector to mbti personality\n",
    "    s = \"\"\n",
    "    for i, l in enumerate(personality):\n",
    "        s += Pers_list[i][l]\n",
    "    return s\n",
    "\n",
    "list_personality_bin = np.array([translate_personality(p) for p in data.type])\n",
    "print(\"Binarize MBTI list: \\n%s\" % list_personality_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47997,
     "status": "ok",
     "timestamp": 1623140117905,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "KcIdQpA0L30X",
    "outputId": "09678940-fd17-4bbc-85f7-c96c1c17b97c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Febian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 8675 rows\n",
      "500 of 8675 rows\n",
      "1000 of 8675 rows\n",
      "1500 of 8675 rows\n",
      "2000 of 8675 rows\n",
      "2500 of 8675 rows\n",
      "3000 of 8675 rows\n",
      "3500 of 8675 rows\n",
      "4000 of 8675 rows\n",
      "4500 of 8675 rows\n",
      "5000 of 8675 rows\n",
      "5500 of 8675 rows\n",
      "6000 of 8675 rows\n",
      "6500 of 8675 rows\n",
      "7000 of 8675 rows\n",
      "7500 of 8675 rows\n",
      "8000 of 8675 rows\n",
      "8500 of 8675 rows\n",
      "8675 of 8675 rows\n",
      "Example :\n",
      "\n",
      "Post before preprocessing:\n",
      "\n",
      "     enfp  intj moments   sportscenter    plays   pranks   what  been  most lifechanging experience  your life eostokenquest        repeat  most  today eostokendot      perc experience immerse  eostokendot     last thing  infj friend posted   facebook before committing suicide  next  eostokendot  rest  peace     hello enfj eostokendot  sorry  hear  your distress eostokendot   only natural   relationship    perfection   time  every moment  existence eostokendot    figure  hard times  times  growth  eostokendot  eostokendot  eostokendot          eostokendot  eostokendot  eostokendot    welcome  stuff eostokendot     game eostokendot   eostokendot  match eostokendot    prozac wellbrutin  least thirty minutes  moving your legs   dont mean moving them while sitting  your same desk chair weed  moderation maybe  edibles   healthier alternative eostokendot  eostokendot  eostokendot    basically come  with three items youve determined that each type  whichever types  want   would more than likely  given each types cognitive functions  whatnot when left  eostokendot  eostokendot  eostokendot     things  moderation eostokendot   sims  indeed  video game   good   that eostokendot  note  good   that  somewhat subjective  that    completely promoting  death   given  eostokendot  eostokendot  eostokendot    dear enfp  what were your favorite video games growing   what  your  current favorite video games eostokenquest  cool      appears    late eostokendot     theres someone  there  everyone eostokendot    wait eostokendot  eostokendot  eostokendot   thought confidence   good thing eostokendot     just cherish  time  solitude   revel within  inner world more whereas most other time   workin eostokendot  eostokendot  eostokendot  just enjoy   time while   eostokendot  dont worry people will always  around  eostokendot  eostokendot  eostokendot     entp ladies eostokendot  eostokendot  eostokendot   youre into  complimentary personalitywell  eostokendot     eostokendot  eostokendot  eostokendot  when your main social outlet  xbox live conversations  even then  verbally fatigue quickly eostokendot      really   part from        banned because this thread requires    eostokendot     high  backyard roast   marshmellows  backyard while conversing over something intellectual followed  massages  kisses eostokendot          banned   many   that sentence eostokendot   could  eostokenexs  think    eostokenexs    banned  watching movies   corner with  dunces eostokendot    banned because health class clearly taught  nothing about peer pressure eostokendot    banned   whole host  reasons eostokenexs        baby deer  left  right munching   beetle   middle eostokendot    using their  blood  cavemen diary todays latest happenings  their designated cave diary wall eostokendot        eostokendot  eostokendot  eostokendot     pokemon world   infj society  everyone becomes  optimist                artists  artists because they draw eostokendot    idea that counts  forming something  your  eostokendot  eostokendot  eostokendot  like  signature eostokendot    welcome   robot ranks person  downed  selfesteem     avid signature artist like herself eostokendot  proud   banned  taking   room under   eostokendot   gotta learn  share with  roaches eostokendot      banned  being  much   thundering grumbling kind  storm eostokendot  eostokendot  eostokendot   eostokendot     eostokendot  eostokendot  eostokendot   high school music  havent heard  ages eostokendot       failed  public speaking class   years    sort  learned what  could  better were     that position again eostokendot    part   failure  just overloading myself with  eostokendot  eostokendot  eostokendot     like this persons mentality eostokendot    confirmed intj    eostokendot    move   denver area  start   life  myself eostokendot     \n",
      "\n",
      "Post after preprocessing:\n",
      "\n",
      "    moment sportscenter play prank lifechanging experience life eostokenquest repeat today eostokendot perc experience immerse eostokendot last thing  friend posted facebook committing suicide next eostokendot rest peace hello  eostokendot sorry hear distress eostokendot natural relationship perfection time every moment existence eostokendot figure hard time time growth eostokendot eostokendot eostokendot eostokendot eostokendot eostokendot welcome stuff eostokendot game eostokendot eostokendot match eostokendot prozac wellbrutin least thirty minute moving leg dont mean moving sitting desk chair weed moderation maybe edible healthier alternative eostokendot eostokendot eostokendot basically come three item youve determined type whichever type want would likely given type cognitive function whatnot left eostokendot eostokendot eostokendot thing moderation eostokendot sims indeed video game good eostokendot note good somewhat subjective completely promoting death given eostokendot eostokendot eostokendot dear  favorite video game growing current favorite video game eostokenquest cool appears late eostokendot there someone everyone eostokendot wait eostokendot eostokendot eostokendot thought confidence good thing eostokendot cherish time solitude revel within inner world whereas time workin eostokendot eostokendot eostokendot enjoy time eostokendot dont worry people always around eostokendot eostokendot eostokendot  lady eostokendot eostokendot eostokendot youre complimentary personalitywell eostokendot eostokendot eostokendot eostokendot main social outlet xbox live conversation even verbally fatigue quickly eostokendot really part banned thread requires eostokendot high backyard roast marshmellows backyard conversing something intellectual followed massage kiss eostokendot banned many sentence eostokendot could eostokenexs think eostokenexs banned watching movie corner dunce eostokendot banned health class clearly taught nothing peer pressure eostokendot banned whole host reason eostokenexs baby deer left right munching beetle middle eostokendot using blood caveman diary today latest happening designated cave diary wall eostokendot eostokendot eostokendot eostokendot pokemon world  society everyone becomes optimist artist artist draw eostokendot idea count forming something eostokendot eostokendot eostokendot like signature eostokendot welcome robot rank person downed selfesteem avid signature artist like eostokendot proud banned taking room eostokendot gotta learn share roach eostokendot banned much thundering grumbling kind storm eostokendot eostokendot eostokendot eostokendot eostokendot eostokendot eostokendot high school music havent heard age eostokendot failed public speaking class year sort learned could better position eostokendot part failure overloading eostokendot eostokendot eostokendot like person mentality eostokendot confirmed  eostokendot move denver area start life eostokendot \n",
      "\n",
      "MBTI before preprocessing:\n",
      "\n",
      " INFJ\n",
      "\n",
      "MBTI after preprocessing:\n",
      "\n",
      " [0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "def pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True):\n",
    "  list_personality = []\n",
    "  list_posts = []\n",
    "  len_data = len(data)\n",
    "  i=0\n",
    "  \n",
    "  for row in data.iterrows():\n",
    "      #check code working \n",
    "      i+=1\n",
    "      if (i % 500 == 0 or i == 1 or i == len_data):\n",
    "        print(\"%s of %s rows\" % (i, len_data))\n",
    "\n",
    "      #Remove and clean comments\n",
    "      posts = row[1].posts\n",
    "\n",
    "      #Remove url links \n",
    "      temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n",
    "\n",
    "      #Remove Non-words - keep only words\n",
    "      temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "\n",
    "      # Remove spaces > 1\n",
    "      temp = re.sub(' +', ' ', temp).lower()\n",
    "\n",
    "      #Remove multiple letter repeating words\n",
    "      temp = re.sub(r'([a-z])\\1{2,}[\\s|\\w]*', '', temp)\n",
    "\n",
    "      #Remove stop words\n",
    "      if remove_stop_words:\n",
    "          temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in useless_words])\n",
    "      else:\n",
    "          temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n",
    "          \n",
    "      #Remove MBTI personality words from posts\n",
    "      if remove_mbti_profiles:\n",
    "          for t in unique_type_list:\n",
    "              temp = temp.replace(t,\"\")\n",
    "\n",
    "      # transform mbti to binary vector\n",
    "      type_labelized = translate_personality(row[1].type)\n",
    "      #or use lab_encoder.transform([row[1].type])[0]\n",
    "      list_personality.append(type_labelized)\n",
    "      # the cleaned data temp is passed here\n",
    "      list_posts.append(temp)\n",
    "\n",
    "  # returns the result\n",
    "  list_posts = np.array(list_posts)\n",
    "  list_personality = np.array(list_personality)\n",
    "  return list_posts, list_personality\n",
    "\n",
    "list_posts, list_personality  = pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True)\n",
    "\n",
    "print(\"Example :\")\n",
    "print(\"\\nPost before preprocessing:\\n\\n\", data.posts[0])\n",
    "print(\"\\nPost after preprocessing:\\n\\n\", list_posts[0])\n",
    "print(\"\\nMBTI before preprocessing:\\n\\n\", data.type[0])\n",
    "print(\"\\nMBTI after preprocessing:\\n\\n\", list_personality[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1623140136765,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "D3PfcoxKMJO7",
    "outputId": "a63bb009-765c-4bdd-e17c-41df72e57a1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of posts = 8675  and No. of Personalities = 4 \n"
     ]
    }
   ],
   "source": [
    "nRow, nCol = list_personality.shape\n",
    "print(f'No. of posts = {nRow}  and No. of Personalities = {nCol} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5170,
     "status": "ok",
     "timestamp": 1623140143802,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "U_7irn8rNtOQ",
    "outputId": "4ddef81b-6c59-435d-96d7-cc7b9e00432f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CountVectorizer :\n",
      "10 feature names can be seen below\n",
      "[(0, 'ability'), (1, 'able'), (2, 'absolutely'), (3, 'across'), (4, 'action'), (5, 'actually'), (6, 'advice'), (7, 'agree'), (8, 'almost'), (9, 'alone')]\n",
      "\n",
      "Using Tf-idf :\n",
      "Now the dataset size is as below\n",
      "(8675, 533)\n"
     ]
    }
   ],
   "source": [
    "# Vectorizing the database posts to a matrix of token counts for the model\n",
    "cntizer = CountVectorizer(analyzer=\"word\", \n",
    "                             max_features=1000,  \n",
    "                             max_df=0.7,\n",
    "                             min_df=0.1) \n",
    "# the feature should be made of word n-gram \n",
    "# Learn the vocabulary dictionary and return term-document matrix\n",
    "print(\"Using CountVectorizer :\")\n",
    "X_cnt = cntizer.fit_transform(list_posts)\n",
    "\n",
    "#The enumerate object yields pairs containing a count and a value (useful for obtaining an indexed list)\n",
    "feature_names = list(enumerate(cntizer.get_feature_names()))\n",
    "print(\"10 feature names can be seen below\")\n",
    "print(feature_names[0:10])\n",
    "\n",
    "# For the Standardization or Feature Scaling Stage :-\n",
    "# Transform the count matrix to a normalized tf or tf-idf representation\n",
    "tfizer = TfidfTransformer()\n",
    "\n",
    "# Learn the idf vector (fit) and transform a count matrix to a tf-idf representation\n",
    "print(\"\\nUsing Tf-idf :\")\n",
    "\n",
    "print(\"Now the dataset size is as below\")\n",
    "X_tfidf =  tfizer.fit_transform(X_cnt).toarray()\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1623140165855,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "MadIMNhpNvjL",
    "outputId": "cb0cb491-93dc-4fdc-f165-d45bce52e503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: Introversion (I) / Extroversion (E)\n",
      "NS: Intuition (N) / Sensing (S)\n",
      "FT: Feeling (F) / Thinking (T)\n",
      "JP: Judging (J) / Perceiving (P)\n"
     ]
    }
   ],
   "source": [
    "personality_type = [ \"IE: Introversion (I) / Extroversion (E)\", \"NS: Intuition (N) / Sensing (S)\", \n",
    "                   \"FT: Feeling (F) / Thinking (T)\", \"JP: Judging (J) / Perceiving (P)\" ]\n",
    "\n",
    "for l in range(len(personality_type)):\n",
    "    print(personality_type[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1623140191105,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "rF8Nv3FEN1Tf",
    "outputId": "8d719dff-4651-4495-8812-370b4669ac5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MBTI personality type : INFJ\n",
      "Y : Binarized MBTI 1st row: [0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"For MBTI personality type : %s\" % translate_back(list_personality[0,:]))\n",
    "print(\"Y : Binarized MBTI 1st row: %s\" % list_personality[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1041,
     "status": "ok",
     "timestamp": 1623140197028,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "8snnkdmYN-C0",
    "outputId": "8b3ab5b1-c68d-456c-fe14-700efac1e679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: Introversion (I) / Extroversion (E) Accuracy: 77.23%\n",
      "NS: Intuition (N) / Sensing (S) Accuracy: 86.06%\n",
      "FT: Feeling (F) / Thinking (T) Accuracy: 71.36%\n",
      "JP: Judging (J) / Perceiving (P) Accuracy: 61.82%\n"
     ]
    }
   ],
   "source": [
    "# Posts in tf-idf representation\n",
    "X = X_tfidf\n",
    "# Logistic Regression for MBTI dataset\n",
    "for l in range(len(personality_type)):\n",
    "\n",
    "    Y = list_personality[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "    # fit model on training data\n",
    "    model = LogisticRegression() \n",
    "    sklearn_model = model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w-WhEKoh3A4"
   },
   "source": [
    "##**Transfer SK-Learn Model to Tensorflow Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 890,
     "status": "ok",
     "timestamp": 1623140224762,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "dxRqI4Bw0zID",
    "outputId": "9b5fae19-a77b-47df-9cfc-23ca8c40d244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Febian\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "tf_model = tf.keras.models.Sequential()\n",
    "tf_model.add(tf.keras.Input(shape=(533,)))\n",
    "tf_model.add(tf.keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 535,
     "status": "ok",
     "timestamp": 1623140229079,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "gPYIoSE9XRbs",
    "outputId": "a1fca4dd-2c03-4afb-c9d2-625135377126"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'AssignVariableOp_1' shape=(1,) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_model.layers[0].weights[0].assign(sklearn_model.coef_.transpose())\n",
    "tf_model.layers[0].bias.assign(sklearn_model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 809,
     "status": "ok",
     "timestamp": 1623140232049,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "0vxIk2AgdFSq"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-dbe3da5fd585>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msklearn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "assert np.all((tf_model(X_train) > 0)[:, 0].numpy() == sklearn_model.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIXlhEGYjJFE"
   },
   "source": [
    "## **Saved Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1073,
     "status": "ok",
     "timestamp": 1623140236148,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "O3Bp_FrndS2G",
    "outputId": "6940784e-4fbf-4ed1-8365-387a67e11087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: save_model/assets\n"
     ]
    }
   ],
   "source": [
    "export_dir = 'save_model/'\n",
    "tf.saved_model.save(tf_model, export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2fuvdazu3T3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jssVUfznjMwJ"
   },
   "source": [
    "## **Convert Model to TF Lite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1623140260197,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "Dzr8xvYOday1"
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 508,
     "status": "ok",
     "timestamp": 1623140362552,
     "user": {
      "displayName": "Wahyu Nurlaila Awaliyah M0040228",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjOL16u4GwPS0hgTrSizcdXLdUg3aWFbedgSHvW=s64",
      "userId": "04166729798987087216"
     },
     "user_tz": -420
    },
    "id": "6TrgPmpEdljo",
    "outputId": "13712ae3-65a8-41a2-a712-30f64a49e37c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3024"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "tflite_model_file = pathlib.Path('classifier_model.tflite')\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Convert Model to TF Lite Using TF Lite Model Maker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "K_DqUYaost7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Cache entry deserialization failed, entry ignored\n",
      "ERROR: httplib2 0.19.1 has requirement pyparsing<3,>=2.4.2, but you'll have pyparsing 2.4.0 which is incompatible.\n",
      "ERROR: google-api-core 1.30.0 has requirement google-auth<2.0dev,>=1.25.0, but you'll have google-auth 1.24.0 which is incompatible.\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\users\\\\febian\\\\anaconda3\\\\lib\\\\site-packages\\\\~5py\\\\defs.cp37-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install -q tflite-model-maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import text_classifier\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker.text_classifier import AverageWordVecSpec\n",
    "from tflite_model_maker.text_classifier import DataLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNibRP4/Ch3+f5T+iWgnwW8",
   "collapsed_sections": [],
   "mount_file_id": "1A92891JjvfhYhlFwlwgIPDepdj6bu93_",
   "name": "model-classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
